# RacoAI_task
Bengali Empathetic Conversations – LLaMA Fine-Tuning
Completed Work
	•	Preprocessed the Bengali Empathetic Conversations dataset.
	•	Cleaned and formatted the dataset into JSONL for LLM fine-tuning.
	•	Set up the required project structure (data, src, notebooks).
	•	Implemented basic OOP modules:
			DatasetProcessor
			UnslothTrainer (initial setup)
			Evaluator (initial structure)
	•	Prepared the preprocessing notebook (racoai-preprocessing.ipynb).
	•	Uploaded processed dataset and source code to Kaggle.
Not Completed
	•	Full fine-tuning of LLaMA 3.1–8B using Unsloth/LoRA.
	•	Evaluation (BLEU, ROUGE, perplexity).
	•	Final model outputs and analysis.
Notes
The full fine-tuning task could not be completed due to unresolved issues with the Unsloth library on Kaggle. All completed work, including preprocessing, dataset formatting, and initial module setup, is included.
Thank you to the RacoAI team for the opportunity.

